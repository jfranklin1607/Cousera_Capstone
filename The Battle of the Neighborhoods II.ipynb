{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "\nPart 2 Web scrapping of Population and Demographics data of New York city from Wikipedia\nA : POPULATION DATA\nWeb scrapping of Population data from wikipedia page - https://en.wikipedia.org/wiki/New_York_City\nDownload all the dependencies that is needed."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "import numpy as np # library to handle data in a vectorized manner\n\nimport pandas as pd # library for data analsysis\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\nimport json # library to handle JSON files\n\n!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\nfrom geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n\nimport requests # library to handle requests\nfrom pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n\n# Matplotlib and associated plotting modules\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\nimport matplotlib.pyplot as plt\n\n# conda install -c anaconda beautiful-soup --yes\nfrom bs4 import BeautifulSoup # package for parsing HTML and XML documents\n\nimport csv # implements classes to read and write tabular data in CSV form\n\nprint('Libraries imported.')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "website_url = requests.get('https://en.wikipedia.org/wiki/Demographics_of_New_York_City').text\nsoup = BeautifulSoup(website_url,'lxml')\ntable = soup.find('table',{'class':'wikitable sortable'})\n#print(soup.prettify())\n\nheaders = [header.text for header in table.find_all('th')]\n\ntable_rows = table.find_all('tr')        \nrows = []\nfor row in table_rows:\n   td = row.find_all('td')\n   row = [row.text for row in td]\n   rows.append(row)\n\nwith open('BON2_POPULATION1.csv', 'w') as f:\n   writer = csv.writer(f)\n   writer.writerow(headers)\n   writer.writerows(row for row in rows if row)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "Pop_data=pd.read_csv('BON2_POPULATION1.csv')\nPop_data.drop(Pop_data.columns[[7,8,9,10,11]], axis=1,inplace=True)\nprint('Data downloaded!')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "Pop_data.columns = Pop_data.columns.str.replace(' ', '')\nPop_data.columns = Pop_data.columns.str.replace('\\'','')\nPop_data.rename(columns={'Borough':'persons_sq_mi','County':'persons_sq_km'}, inplace=True)\nPop_data"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "Pop_data.rename(columns = {'NewYorkCitysfiveboroughsvte\\n' : 'Borough',\n                   'Jurisdiction\\n':'County',\n                   'Population\\n':'Estimate_2017', \n                   'Landarea\\n':'square_miles',\n                    'Density\\n':'square_km'}, inplace=True)\nPop_data"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "Pop_data['Borough']=Pop_data['Borough'].replace(to_replace='\\n', value='', regex=True)\nPop_data['County']=Pop_data['County'].replace(to_replace='\\n', value='', regex=True)\nPop_data['Estimate_2017']=Pop_data['Estimate_2017'].replace(to_replace='\\n', value='', regex=True)\nPop_data['square_miles']=Pop_data['square_miles'].replace(to_replace='\\n', value='', regex=True)\nPop_data['square_km']=Pop_data['square_km'].replace(to_replace='\\n', value='', regex=True)\nPop_data['persons_sq_mi']=Pop_data['persons_sq_mi'].replace(to_replace='\\n', value='', regex=True)\nPop_data['persons_sq_km']=Pop_data['persons_sq_km'].replace(to_replace='\\n', value='', regex=True)\nPop_data"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "Pop_data.loc[5:,['persons_sq_mi','persons_sq_km']] = Pop_data.loc[2:,['persons_sq_mi','persons_sq_km']].shift(1,axis=1)\nPop_data.loc[5:,['square_km','persons_sq_mi']] = Pop_data.loc[2:,['square_km','persons_sq_mi']].shift(1,axis=1)\nPop_data.loc[5:,['square_miles','square_km']] = Pop_data.loc[2:,['square_miles','square_km']].shift(1,axis=1)\nPop_data.loc[5:,['Estimate_2017','square_miles']] = Pop_data.loc[2:,['Estimate_2017','square_miles']].shift(1,axis=1)\nPop_data.loc[5:,['County','Estimate_2017']] = Pop_data.loc[2:,['County','Estimate_2017']].shift(1,axis=1)\nPop_data.loc[5:,['Borough','County']] = Pop_data.loc[2:,['Borough','County']].shift(1,axis=1)\nPop_data"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "Pop_data = Pop_data.fillna('')\nPop_data"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "i = Pop_data[((Pop_data.County == 'Sources: [2] and see individual borough articles'))].index\nPop_data.drop(i)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "Pop_data.to_csv('BON2_POPULATION.csv',index=False)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "website_url = requests.get('https://en.wikipedia.org/wiki/New_York_City').text\nsoup = BeautifulSoup(website_url,'lxml')\ntable = soup.find('table',{'class':'wikitable sortable collapsible'})\n#print(soup.prettify())\n\nheaders = [header.text for header in table.find_all('th')]\n\ntable_rows = table.find_all('tr')        \nrows = []\nfor row in table_rows:\n   td = row.find_all('td')\n   row = [row.text for row in td]\n   rows.append(row)\n\nwith open('NYC_DEMO.csv', 'w') as f:\n   writer = csv.writer(f)\n   writer.writerow(headers)\n   writer.writerows(row for row in rows if row)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "Demo_data=pd.read_csv('NYC_DEMO.csv')\nprint('Data downloaded!')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "Demo_data"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "Demo_data.rename(columns = {'2010[237]' : '2010',\n                   '1990[239]':'1990',\n                   '1970[239]':'1970', \n                   '1940[239]\\n':'1940',\n                    }, inplace=True)\nDemo_data"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "Demo_data.columns"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "Demo_data.columns = Demo_data.columns.str.replace(' ', '')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "Demo_data= Demo_data.replace('\\n',' ', regex=True)\nDemo_data"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "Demo_data['1970'] = Demo_data['1970'].str.rstrip('[240]')\nDemo_data"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "Demo_data.to_csv('BON2_DEMOGRAPHICS.csv',index=False)"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}